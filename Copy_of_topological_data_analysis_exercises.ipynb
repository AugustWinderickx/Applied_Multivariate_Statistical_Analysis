{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AugustWinderickx/Applied_Multivariate_Statistical_Analysis/blob/main/Copy_of_topological_data_analysis_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topological Data Analysis\n",
        "\n",
        "In this tutorial you will learn how to apply common topological data analysis (TDA) tools for explorating high-dimensional data. TDA has many uses within data analysis. We will focus on exploration, which is particularly powerful when combined with interactive visualizations. TDA can also be used for feature extraction (i.e., summarize complex data by their topological features), guide feature selection for machine learning (i.e., determine which features distinguish sub-populations within the data), and model interpretation (i.e., investigate how a model treats different aspects of the input-space)."
      ],
      "metadata": {
        "id": "PwcyUZ_1Fpak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical matters\n",
        "\n",
        "To run this notebook, either on Google Colaborate or locally, you will have to install several packages. Run the code cells in this section and make sure to restart the kernel when asked. This may take a couple of minutes..."
      ],
      "metadata": {
        "id": "XKMnIdovgeO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install giotto-tda multi-mst returns"
      ],
      "metadata": {
        "id": "QB48BRkLg1F2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports\n",
        "\n",
        "General imports, don't forget to evaluate this cell :) Also: this may take a while.\n",
        "\n",
        "If this crashes, comment the lines with `multi_mst`. We will then have to skip that specific exercise towards the end of the notebook."
      ],
      "metadata": {
        "id": "n3MZk73zmqjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from multi_mst.k_mst_descent import KMSTDescent\n",
        "from multi_mst.noisy_mst import NoisyMST\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "import json\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from returns.pipeline import flow\n",
        "import random\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse.csgraph import minimum_spanning_tree"
      ],
      "metadata": {
        "id": "ScivzmqZmrx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The data\n",
        "We will use two small datasets to play with. One is a point-cloud of a horse, and the other is the dataset used in the lecture."
      ],
      "metadata": {
        "id": "DMsJ-tzag1Ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The horse"
      ],
      "metadata": {
        "id": "cOIa2vMGj0_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horse_data = pd.read_csv(\"http://aida-lab.be/assets/horse.csv\")\n",
        "\n",
        "# We have to take a subsample of the data, otherwise the networkx plotting crashes...\n",
        "# Comment this line if you want to export to CSV and load in gephi.\n",
        "horse_data = horse_data.sample(n=2000, random_state=42)\n",
        "\n",
        "# We'll also add an `id` column to each row which we'll need later.\n",
        "horse_data['id'] = range(0, len(horse_data))"
      ],
      "metadata": {
        "id": "--UJZiwOnxy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horse_data"
      ],
      "metadata": {
        "id": "5mSeyLLkoKty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(horse_data,x=\"x\",y=\"y\")"
      ],
      "metadata": {
        "id": "Hm2fqs9Hi8oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(horse_data,x=\"z\",y=\"y\")"
      ],
      "metadata": {
        "id": "dFbMoSyDi-3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So apparently:\n",
        "* `x` is left-right\n",
        "* `y` is bottom-top\n",
        "* `z` is back-front"
      ],
      "metadata": {
        "id": "l3XMFNo9jRCS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The circle(s)\n",
        "You will recognise this data from the lecture, but not right away...\n",
        "\n",
        "In this case, the graph was already created beforehand, but we will first plot some of the features."
      ],
      "metadata": {
        "id": "Y2nYLPCulSnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circles_data = pd.read_csv(\"http://aida-lab.be/assets/circles.csv\")\n",
        "\n",
        "circles_data"
      ],
      "metadata": {
        "id": "Rtq_ZIvKleUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at what this data looks like. It's weird, but don't worry: it will make sense later."
      ],
      "metadata": {
        "id": "0LniFvOYo4lY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(circles_data, x=\"x\", y=\"y\", hue=\"colour_hex\", size=\"r\", legend=False)"
      ],
      "metadata": {
        "id": "N2M8fJoUohNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From high-dimensional data to a network\n",
        "## Preamble\n",
        "### Different graph formats\n",
        "Depending on the tool that you want to use for _visualising_ the networks, you will need different formats.\n",
        "\n",
        "In this notebook, we will always create nodes and links as arrays. For example:\n",
        "\n",
        "`nodes` is an array of arrays. In case of the horse, containing `x`, `y`, and `z` values:\n",
        "```\n",
        "[[-1.06305e-02,  8.44801e-01,  4.67552e-01],\n",
        " [-2.40724e-02,  8.10356e-01,  4.72199e-01],\n",
        " [-1.80217e-03,  2.64261e-01, -5.05758e-01],\n",
        " [-3.20978e-02,  7.61185e-01,  3.15659e-01],\n",
        " [2.86350e-03,  7.25523e-01,  1.60297e-01]]\n",
        "```\n",
        "\n",
        "`links` is an array of 2D-arrays, containing the _source_ and _target_ node. That node is identified by its index in the `nodes` array, **starting from 0**:\n",
        "```\n",
        "[[0,1],[0,2],[1,2],[2,3]]\n",
        "```\n",
        "\n",
        "#### CSV\n",
        "Let's start with the simplest output format: the nodes and links are stored in separate CSV files. You will need these, if you for example want to show your data in [gephi](http://www.gephi.org).\n",
        "\n",
        "To export your data in 2 CSV files, it's easiest to create a Pandas dataframe for each first. In case of the horse data, we _already_ have it in that format, so we don't need to do anything:\n",
        "```\n",
        "horse_df.to_csv('nodes.csv', index=False)\n",
        "\n",
        "links_df = pd.DataFrame(links, columns=[\"source\",\"target\"])\n",
        "links_df.to_csv('links.csv', index=False)\n",
        "```\n",
        "\n",
        "\n",
        "#### D3-based format\n",
        "For some other applications, we need a single JSON file containing nodes and links. It should look like this:\n",
        "\n",
        "```\n",
        "{ \"nodes\": [\n",
        "    {\"id\": 0, \"x\":1, \"y\":2},\n",
        "    {\"id\": 1, \"x\":3, \"y\":4},\n",
        "    {\"id\": 2, \"x\":5, \"y\":6},\n",
        "    {\"id\": 3, \"x\":7, \"y\":8}],\n",
        "  \"links\": [\n",
        "    {\"source\":0,\"target\":1},\n",
        "    {\"source\":0,\"target\":2},\n",
        "    {\"source\":1,\"target\":2},\n",
        "    {\"source\":2,\"target\":3}]\n",
        "}\n",
        "```\n",
        "\n",
        "Given that we have the structures that we described above, we can create the nodes as follows:\n",
        "```\n",
        "nodes_as_json = my_dataframe.to_dict(orient=\"records\")\n",
        "```\n",
        "\n",
        "Similarly, to create the links:\n",
        "```\n",
        "links_as_json = [{'source': item[0], 'target': item[1]} for item in links]\n",
        "```\n",
        "\n",
        "To combine these in a single datastructure:\n",
        "```\n",
        "my_graph = {\"nodes\": nodes_as_json, \"links\": links_as_json}\n",
        "```\n",
        "\n",
        "And finally save them to a file\n",
        "```\n",
        "with open('my_graph.json', 'w') as f:\n",
        "    json.dump(my_graph, f)\n",
        "```\n",
        "\n",
        "### Where are files saved?\n",
        "You can find your saved file in Google colab if you click on the \"folder\" icon on the left."
      ],
      "metadata": {
        "id": "AtQYhPkyK9XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From Pandas data frame to graph nodes\n",
        "horse_nodes = horse_data.to_dict(orient=\"records\")\n",
        "horse_nodes[:5]"
      ],
      "metadata": {
        "id": "ZoymnAXQrcfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## kNN networks\n",
        "A simple way to construct a network representation from a point cloud is to add edges between each node's k-nearest neighbours. The idea of this approach is that near neighbours better capture the local manifold of the data than just the distances. On the other hand, KNN networks lose density information, as all points will get the same number of edges in the network.\n",
        "\n",
        "The code below constructs a KNN network on top of the data's minimum spanning tree and computes the correlation of the distances in the network with the point cloud distances."
      ],
      "metadata": {
        "id": "MLWaY-H-nflK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horse_distances = pdist(horse_data)\n",
        "horse_distance_matrix = squareform(horse_distances)"
      ],
      "metadata": {
        "id": "ki9W5EYjppIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_knn_links(distance_matrix):\n",
        "  k = 5\n",
        "  knn_links = []\n",
        "\n",
        "  for i in range(len(distance_matrix[0])):\n",
        "      dist_i = distance_matrix[i]\n",
        "\n",
        "      # Get the indices of the k nearest neighbors (excluding the point itself)\n",
        "      nearest_indices = np.argsort(dist_i)[1:k+1]  # Exclude the point itself (index 0)\n",
        "\n",
        "      for j in nearest_indices:\n",
        "          knn_links.append([i,j,dist_i[j]])\n",
        "\n",
        "  return knn_links"
      ],
      "metadata": {
        "id": "GDPyBlvJnrj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horse_knn_links = create_knn_links(horse_distance_matrix)"
      ],
      "metadata": {
        "id": "MazNOZntpccg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting using `networkx`\n",
        "To draw the network, we can\n",
        "\n",
        "1. export to 2 CSV files and upload in [gephi](www.gephi.org).\n",
        "2. export as JSON file and use a custom tool (we won't do that here).\n",
        "3. show the graph _within this notebook_, using `networkx`.\n",
        "\n",
        "Below you can find the code to draw the graph using `networkx`. **CAUTION**, plots generated by `networkx` are generally good if the graphs are small (10-20 nodes). We're working with much larger data, so the result will be far from ideal."
      ],
      "metadata": {
        "id": "E8Gpw2HDb1wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_networkx(nodes, links):\n",
        "  G = nx.Graph()\n",
        "\n",
        "  for node in nodes:\n",
        "    G.add_node(node[\"id\"])\n",
        "\n",
        "  for link in links:\n",
        "    G.add_edge(link[0],link[1])\n",
        "\n",
        "  return G"
      ],
      "metadata": {
        "id": "Jirlt23VwFYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can plot. The following code block may take some time..."
      ],
      "metadata": {
        "id": "LTaGAxHKwni9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G = create_networkx(horse_nodes, horse_knn_links)\n",
        "pos = nx.forceatlas2_layout(G, max_iter=200)  # positions for all nodes\n",
        "# pos = nx.arf_layout(G)\n",
        "nx.draw(G, pos, node_color='steelblue', alpha=0.5, edge_color='gray', node_size=20)\n",
        "plt.title('Horse graph')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kKKBp0p1nmf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting using a custom tool\n",
        "NOTE: This is just for your reference; you won't have to do this yourself.\n",
        "\n",
        "To convert our data into the D3 format and save it as a JSON file:"
      ],
      "metadata": {
        "id": "GJmSHM1xcikw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "67khajrkkrCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links_as_json = [{'source': int(item[0]), 'target': int(item[1])} for item in horse_knn_links]"
      ],
      "metadata": {
        "id": "c9QAQ2O1NE4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horse_graph = {\"nodes\": horse_nodes, \"links\": links_as_json}"
      ],
      "metadata": {
        "id": "o7ehAcB2Q0lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('horse_graph.json', 'w') as f:\n",
        "    json.dump(horse_graph, f)"
      ],
      "metadata": {
        "id": "c-SQVZrZdIyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting using gephi\n",
        "#### Installing/using gephi\n",
        "\n",
        "Gephi can be downloaded from www.gephi.org.\n",
        "\n",
        "To load a network, first load the **nodes** from a CSV file. In \"File\", click \"**Import spreadsheet**\". In the last step, choose \"Undirected\" for \"Graph Type\" and \"New workspace\".\n",
        "\n",
        "To load the **edges**, again \"File\" and \"Import spreadsheet\". Here also choose \"Undirected\" for \"Graph Type\", but \"**Append to existing workspace**\" instead of \"New workspace\".\n",
        "\n",
        "To draw the network, select the \"**Choose a layout**\" dropdown box on the left, and choose e.g. ForceAtlas2. Try out different layouts.\n",
        "\n",
        "You can change the **size and colour of the nodes** (and links) on the top left. Choose \"unique\" if you want to give all nodes the same colour or size; choose \"partition\" if you have categories; choose \"Ranking\" if you want to use a numerical feature."
      ],
      "metadata": {
        "id": "yF2PYIflcp5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the files for use in gephi\n",
        "Gephi will need two CSV files."
      ],
      "metadata": {
        "id": "siFcr30t419A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horse_data.to_csv('nodes.csv', index=False)\n",
        "\n",
        "df_links = pd.DataFrame(horse_knn_links, columns=[\"source\",\"target\",\"distance\"])\n",
        "df_links.to_csv('links.csv', index=False)"
      ],
      "metadata": {
        "id": "vO6tLijuOX0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-MST networks\n",
        "Here we generate small deviations from the original distance matrix, and calculate the MST (minimal spanning tree) on that altered matrix. At the end we add those MSTs together.\n",
        "\n",
        "### Using the multi_mst package"
      ],
      "metadata": {
        "id": "O3VCaDO5JZuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multi_mst.noisy_mst import NoisyMST"
      ],
      "metadata": {
        "id": "qojiMujkJ9bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "projector = NoisyMST(num_trees=10, noise_fraction=1.0).fit(horse_data)\n",
        "coo_matrix = projector.graph_.tocoo()\n",
        "sources = coo_matrix.row\n",
        "targets = coo_matrix.col"
      ],
      "metadata": {
        "id": "FazH-o3UKSfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sources[:5]"
      ],
      "metadata": {
        "id": "ndq5X7nRdmrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets[:5]"
      ],
      "metadata": {
        "id": "vznIRsGPdooZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = [[s, t] for s, t in zip(sources, targets)]"
      ],
      "metadata": {
        "id": "7bktSmv9KSMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links[:5]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B_Y32a2LnmWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the code necessary to\n",
        "1. convert the `links` array into a dataframe\n",
        "2. save those links in a CSV file\n",
        "3. save the nodes in a CSV file\n",
        "\n",
        "And then visualise your graph in gephi."
      ],
      "metadata": {
        "id": "VTGgxujKdsz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pY0VHqgwkzWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coding this ourselves\n",
        "We can do a similar thing without using the `multi-mst` package."
      ],
      "metadata": {
        "id": "_DXIEAi256nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = []\n",
        "def alter_distances(distances, max_amount = np.std(distances)/50):\n",
        "    changer = lambda t: t + random.uniform(0, max_amount)\n",
        "    return np.array([changer(d) for d in distances])"
      ],
      "metadata": {
        "id": "fCBBwRe568tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mst(distances):\n",
        "    X = csr_matrix(squareform(distances))\n",
        "    mst = minimum_spanning_tree(X)\n",
        "    return np.nonzero(mst)"
      ],
      "metadata": {
        "id": "m5YDbiRZ6v4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mst_to_links(links, mst):\n",
        "    for i in range(0,len(mst[0])):\n",
        "        # links.append('{\"source\":' + str(mst[0][i]) + ', \"target\":' + str(mst[1][i]) + '}')\n",
        "        links.append([mst[0][i], mst[1][i]])\n",
        "    return links"
      ],
      "metadata": {
        "id": "5O5he1xE6ykO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(i)\n",
        "    flow(\n",
        "        horse_distances,\n",
        "        lambda d: alter_distances(d, max_amount = np.std(horse_distances)),\n",
        "        lambda d: calculate_mst(d),\n",
        "        lambda d: add_mst_to_links(links,d)\n",
        "    )"
      ],
      "metadata": {
        "id": "7ok3Foyp559l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what this looks like in gephi: export as CSVs and load in gephi."
      ],
      "metadata": {
        "id": "rdImimql8QRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "horse_data.to_csv('nodes.csv', index=False)\n",
        "\n",
        "links_df = pd.DataFrame(links, columns=[\"source\",\"target\"])\n",
        "links_df.to_csv('links.csv', index=False)"
      ],
      "metadata": {
        "id": "RKq5FOYO8VSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapper network\n",
        "The mapper networks are fundamentally different from the ones above (see the lecture). Each node in the resulting graph is a _cluster_ of datapoints, not single datapoints."
      ],
      "metadata": {
        "id": "pf_2Lo1WeIrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In spatial domains, topological data analysis is typically used to analyze a signal on a regular grid. For instance, the colour of pixels on an image, or the density of tissue in an CT scan. In these cases, the pixels or voxels provide a notion of connectedness, i.e., pixels that are adjacent to each other are connected. As a result, persistent features can be computed over the signal's values instead of the distance between samples.\n",
        "\n",
        "For point clouds, this is more difficult, because they do not have an inherrent notion of connectivity. Suppose we perform a filtration over some function $f(v)$, that returns a single value for each data point in a point cloud. Then, instead of edges being included in the simplicial complex as the distance threshold increases, now data points are added to the simplicial complex as the threshold on $f$ increases. But how are these data-points connected? Without that information, we cannot compute the topological features.\n",
        "\n",
        "The [Mapper](http://diglib.eg.org/bitstream/handle/10.2312/SPBG.SPBG07.091-100/091-100.pdf?sequence=1&isAllowed=y) algorithm provides a solution to this problem. It is able to analyzing a signal on point-cloud data. The algorithm works by (a) defining overlapping segments over a lens (or filter), i.e. the signal of interest. Then, (b) it clusters the data points within each segment. Mapper does not impose any restrictions on which clustering algorithm can be used. The resulting clustering provides the notion of connectivity, it determines which data points in each segment belong together. The clusters become nodes in the resulting mapper graph. Note, though, that the distance threshold which determines which data points belong to a cluster can vary between the lens's segments! Finally, (c) Edges between these nodes are added based on data point overlap between the nodes. There can be overlap because the segments of the lens have overlap.\n",
        "\n",
        "![Mapper overview](https://gist.githubusercontent.com/JelmerBot/37568240a38c39bf39f20302ed8a130f/raw/9a79ff6e0a2f2c8b79714e5aa4830d99be63a0aa/mapper.png)"
      ],
      "metadata": {
        "id": "eLMkfqcjU5rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below computes and visualizes a mapper network for the horse data set. You can play around with the n_intervals and overlap_frac parameters to change the resultion of segments. You can also change columns of the projection to determine which columns are used as filter. Using more than one column tends to work best for the horse, but in general we do not know in advance which filters produce interesting results! Also check what happens when you do not normalize the input data!"
      ],
      "metadata": {
        "id": "64FyM4FTg467"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtda.mapper import make_mapper_pipeline\n",
        "from gtda.mapper.filter import Projection\n",
        "from gtda.mapper.cover import CubicalCover, OneDimensionalCover\n",
        "from gtda.mapper import plot_static_mapper_graph\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from gtda.mapper.cluster import FirstSimpleGap, FirstHistogramGap\n",
        "\n",
        "# Normalize dimensions with z-score (mean 0 std 1).\n",
        "# The x-dimension is small compared to y and z, so the clustering\n",
        "# does not separate the front-legs and back-legs. By scaling, we\n",
        "# emphazise patterns in dimensions with small values.\n",
        "scaler=StandardScaler()\n",
        "\n",
        "# Define filter function – can be any scikit-learn transformer\n",
        "# The `Projection` class is named confusingly. It does not\n",
        "# project the data, it simply returns the specified columns of\n",
        "# the data. Mapper can deal with more than 1 filter dimension!\n",
        "filter_func=Projection(columns=[1])\n",
        "\n",
        "# Define cover, i.e. the segments over the filters.\n",
        "# Here you can specify the number of segments and their overlap\n",
        "# ratio.\n",
        "cover=OneDimensionalCover(n_intervals=11, overlap_frac=0.2)\n",
        "\n",
        "# Choose clustering algorithm\n",
        "clusterer=FirstSimpleGap()\n",
        "\n",
        "# Initialise pipeline\n",
        "pipe = make_mapper_pipeline(\n",
        "    clustering_preprocessing=scaler,\n",
        "    filter_func=filter_func,\n",
        "    cover=cover,\n",
        "    clusterer=clusterer,\n",
        "    verbose=False,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "# Compute the mapper network and visualize it\n",
        "fig = plot_static_mapper_graph(\n",
        "    pipe, horse_data.to_numpy(), color_data=horse_data[['y', 'z', 'x']]\n",
        ")\n",
        "fig.show(config={'scrollZoom': True})"
      ],
      "metadata": {
        "id": "YE7v44aod91b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look at the `gtda.mapper` documentation at https://giotto-ai.github.io/gtda-docs/0.3.1/modules/mapper.html to see what you can change in the code above.\n",
        "\n",
        "If you use `cover=OneDimensionalCover(n_intervals=11, overlap_frac=0.2)`, can you interpret the picture that comes out?\n",
        "\n",
        "It can difficult to interpret a Mapper network because we do not have control over the clustering. Sometimes we want to increase or reduce the clustering threshold to show more or less nodes per segment. In addition, which filters are used (either dimensions of the data or functions based on the point cloud) also has a large effect on the resulting network."
      ],
      "metadata": {
        "id": "m5FiTxm9gIAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a lens\n",
        "Let's have a look if we add a lens to our network.\n",
        "\n",
        "First, create a multi-MST graph for the `circles_data` dataset. We only want to do that using the `x` and `y` columns, so first need to create a different dataset with just those."
      ],
      "metadata": {
        "id": "_FNhi2rllGgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "circles_data_xy = circles_data.drop([\"r\",\"colour\",\"id\",\"colour_hex\"], axis=1)\n",
        "circles_data_xy"
      ],
      "metadata": {
        "id": "Y11nrk1GpeeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles_nodes = circles_data.to_dict(orient=\"records\")"
      ],
      "metadata": {
        "id": "DG_Mwq7xqZPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles_distances = pdist(circles_data_xy)\n",
        "circles_distance_matrix = squareform(circles_distances)"
      ],
      "metadata": {
        "id": "uDUOW5nb9Ar6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(i)\n",
        "    flow(\n",
        "        circles_distances,\n",
        "        lambda d: alter_distances(d, max_amount = np.std(circles_distances)),\n",
        "        lambda d: calculate_mst(d),\n",
        "        lambda d: add_mst_to_links(links,d)\n",
        "    )"
      ],
      "metadata": {
        "id": "igKDqzwgd9yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles_data.to_csv('nodes.csv', index=False)\n",
        "\n",
        "links_df = pd.DataFrame(links, columns=[\"source\",\"target\"])\n",
        "links_df.to_csv('links.csv', index=False)"
      ],
      "metadata": {
        "id": "1lkr0G6TxYMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we load this data in gephi, we get the following picture:\n",
        "\n",
        "<img src=\"https://aida-lab.pages.gitlab.kuleuven.be/assets/circles_lens_none.png\" alt=\"circles without lens\" height=\"300\" />"
      ],
      "metadata": {
        "id": "8ZGVtgOz32P7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a lens\n",
        "A very simple way to create a lens _post-hoc_, is to remove links that should not be there.\n",
        "\n",
        "For example, the size of the datapoints ranges from 0 to 100. We can remove all links where the difference in node size is larger than 20.\n",
        "\n",
        "We will go through all links, get the nodes, get their sizes and compare them."
      ],
      "metadata": {
        "id": "S3OvKjeXrl5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_difference = 30\n",
        "filtered_links = [\n",
        "    link for link in links\n",
        "    if abs(circles_nodes[link[0]]['colour'] - circles_nodes[link[1]]['colour']) <= max_difference\n",
        "]"
      ],
      "metadata": {
        "id": "xWLQqi3qtB4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(links))\n",
        "print(len(filtered_links))"
      ],
      "metadata": {
        "id": "bV40k3lNtMim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "links_df = pd.DataFrame(filtered_links, columns=[\"source\",\"target\"])\n",
        "links_df.to_csv('filtered_links_colour.csv', index=False)"
      ],
      "metadata": {
        "id": "vEs1nugud9iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the image that we now get from gephi. You clearly see that there is something else going on that you didn't see in the image at the top of this notebook (i.e. where we just plotted `x` and `y`).\n",
        "\n",
        "<img src=\"https://aida-lab.pages.gitlab.kuleuven.be/assets/circles_lens_colour.png\" alt=\"circles with colour lens\" height=\"300\" />"
      ],
      "metadata": {
        "id": "hEWHyLZbHzTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistent homology"
      ],
      "metadata": {
        "id": "cQ_EIiZSI8xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained in the lecture, persistent homology analyzes topological features at multiple scales and determines which features persist across a larger range of these scales. A typical way to compute persistent topological features for point clouds are *Vietoris-Rips complexes*. In 2D, this process can be thought of as spheres covering each data point and introducing an edge when spheres start to overlap as their radius increases. Cliques in the resulting network form *simplices*: edges, triangles, tetrahedrons, and their higher dimensional equivalents. Together, the simplices at a single radius form a *simplicial complex* and the sequence of simplicial complexes over the scale form a *filtration*.\n",
        "\n",
        "![Vietoris-Rips filtration](https://giotto-ai.github.io/gtda-docs/latest/_images/vietoris_rips_point_cloud.gif)\n",
        "\n",
        "[Image from the Giotto TDA website](https://giotto-ai.github.io/gtda-docs/latest/notebooks/persistent_homology_graphs.html)\n",
        "\n",
        "The topological features that we are interested in can be computed from the *simplicial complexes* of a *filtration*. Typically, these topological features correspond to [Betti numbers](https://en.wikipedia.org/wiki/Betti_number) and capture the number of connected components (dimension 0), the number of loops (dimension 1), and number of voids (dimension 2 and higher). Using the filtration, we determine when each feature starts to occur and when it dies. For example, looking at the animation above, you can see at which distance the circle closes, and when it dies because it gets filled in with triangles. The difference between a feature's death and birth is their *persistence*. Generally, we assume that are a few features that are more persistent than the others. These features are interpreted to capture the true underlying shape of the point cloud, while the others are attributed to noise.\n",
        "\n",
        "Note that 0-dimensional persistence diagrams are closely related to single-linkage hierarchical clustering dendrograms. Both constructs describe at which distances certain data-points belong 'together'. Their difference is in their interpretation of a merge. Suppose that there are two connected components, or clusters, that merge at distance some $d$. The dendrogram interprets this merge to mean that both clusters combine and continue to exist as a single entity. The persistence diagram instead sees the merge as the death of one of the components while the other continues to exist."
      ],
      "metadata": {
        "id": "CFvr3vAkgXu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persistent features of a torus\n",
        "\n",
        "In this example, we show how to use Giotto-TDA to compute the persistence diagram of a point cloud, in this case a torus.\n",
        "\n",
        "First, we have to construct the point cloud:"
      ],
      "metadata": {
        "id": "YDc48KZ9VNmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtda.plotting import plot_point_cloud\n",
        "\n",
        "def make_torus(inner_radius = 1, outer_radius = 2, num_samples = 512):\n",
        "  s = np.random.rand(num_samples) * np.pi * 2\n",
        "  t = np.random.rand(num_samples) * np.pi * 2\n",
        "  return np.column_stack([\n",
        "    (outer_radius+inner_radius*np.cos(s))*np.cos(t),\n",
        "    (outer_radius+inner_radius*np.cos(s))*np.sin(t),\n",
        "    inner_radius * np.sin(s)\n",
        "  ])\n",
        "\n",
        "torus = make_torus()\n",
        "plot_point_cloud(torus)"
      ],
      "metadata": {
        "id": "irHbNPsWidEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use Giotto-TDA to compute the persistent features in the first three dimensions. We use a `WeakAlphaPersistence` filtration to speed up the computation. This filtration computes a Vietoris-Rips complex using only the edges in a Delaunay triangulation, which is efficient to create for low-dimensional datasets.\n",
        "\n",
        "Giotto-TDA is designed with machine learning in mind. It expects the input the be a training-set of point-clouds. We, however, only use a single point cloud at the moment. So, we have to pass a list as input `[torus]` and then look at the first diagram in the output `diagrams[0]`. Note how we specify which dimensions should be computed and that we set `reduced_homology = False`. This latter argument makes Giotto-TDA return the infinitly persistent 0-dimensional simplex. Otherwise, it would have been silently removed.\n",
        "\n",
        "In the resulting output, we clearly see the persistent features of a torus: one 0-dimensionional component, two 1-dimensional loops, one 2-dimensional void.\n",
        "\n"
      ],
      "metadata": {
        "id": "fDnT4E0Yuwdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gtda.plotting import plot_diagram\n",
        "from gtda.homology import WeakAlphaPersistence\n",
        "\n",
        "WA = WeakAlphaPersistence(homology_dimensions=[0, 1, 2],\n",
        "                          reduced_homology=False,\n",
        "                          n_jobs=-1)\n",
        "diagrams = WA.fit_transform([torus])\n",
        "\n",
        "plot_diagram(diagrams[0])"
      ],
      "metadata": {
        "id": "1VvL1CWDqWto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to view a persistence diagrams is as a barcode, where each bar represents a topological feature and indicates when the feature started to exist and when it died. The code below constructs barcodes for the torus."
      ],
      "metadata": {
        "id": "Kk0_6qfRrOXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [0, 1, 2]\n",
        "inf_value = 2\n",
        "for i, d in enumerate(ds):\n",
        "  plt.subplot(1, len(ds), i+1)\n",
        "  components = diagrams[0][diagrams[0][:, 2] == d]\n",
        "  for i, pair in enumerate(components):\n",
        "    plt.plot([pair[0], pair[1] if not np.isinf(pair[1]) else inf_value], [i, i], 'k-', linewidth=0.5)\n",
        "  plt.title(f'Barcode for dimension {d}')\n",
        "  plt.xlabel('distance')\n",
        "  plt.ylabel('feature id')\n",
        "  plt.xlim([0, 2.2])\n",
        "\n",
        "  plt.plot([inf_value, inf_value], plt.ylim(), 'k:', linewidth=1)\n",
        "  ticks = np.linspace(0, inf_value, 5)\n",
        "  labels = [\n",
        "    f'{t}' if t != inf_value else 'inf'\n",
        "    for t in ticks\n",
        "  ]\n",
        "  plt.gca().set_xticks(ticks)\n",
        "  plt.gca().set_xticklabels(labels)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.gcf().set_figwidth(10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I2kcMIzG5ruR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persistent features of the horse\n",
        "\n",
        "Using the example above, compute a the persistence diagram for the horse. How many 0, 1, and 2 dimensional features does the horse have? Can you describe how each feature corresponds the horse's shape?"
      ],
      "metadata": {
        "id": "YbhLXMTA44WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_point_cloud(horse_data.to_numpy())"
      ],
      "metadata": {
        "id": "jWQIhfS46Mbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the horse's persistent features here"
      ],
      "metadata": {
        "id": "3jy5KF_r6lCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected there is a single persistent 0-dimensional component. There are no clearly persistent loops, but there is one persistent void."
      ],
      "metadata": {
        "id": "VT3TAAD9-WMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Persistent features of the circles dataset\n",
        "Now do the same thing for the circles data. You will want to use `circles_data_xy` instead of `circles_data` because the latter contains non-numeric data."
      ],
      "metadata": {
        "id": "VbJsgF2HVc3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "IL_0ZHFvVnNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}